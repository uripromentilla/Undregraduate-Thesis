"""
This is a template algorithm on Quantopian for you to adapt and fill in.
"""
from __future__ import print_function
import quantopian.algorithm as algo
import numpy as np
import pandas as pd
import itertools
import random
import math
from math import floor
from collections import defaultdict
from datetime import datetime
from quantopian.pipeline import Pipeline
from quantopian.pipeline.data.builtin import USEquityPricing
from quantopian.pipeline.filters import QTradableStocksUS


def initialize(context):
    """
    Called once at the start of the algorithm.
    """
    
    # def q_learning(env, num_episodes, discount_factor=1.0, alpha=0.1, epsilon=0.01, Q=None):
# HYPERPARAMETERS
    
    # context.my_stock = sid(607) #ATML
    # context.my_stock = sid(700) #BAC
    # context.my_stock = sid(2298) #DHI
    # context.my_stock = sid(3806) #BIIB
    # context.my_stock = sid(5484) #ES
    # context.my_stock = sid(7285) #SYY
    # context.my_stock = sid(10898) #ALB
    context.my_stock = sid(15474) #ETFC
    # context.my_stock = sid(20689) #BLK
    # context.my_stock = sid(21758) #EZU
    # context.my_stock = sid(22443) #GPN
    # context.my_stock = sid(24105) #PALM
    
    # context.my_stock = sid(24465) #ACTG Oscillating
    # context.my_stock = sid(8229) #WMT Uptrend
    # context.my_stock = sid(1595) #CLF Downtrend
    
    context.alpha = 0.9
    context.discount_factor = 0.9
    context.epsilon = 0.4
    context.hallucination = 1
    
    context.num_of_actions = 3
    
    context.duration = 1
    context.temp = 0
    context.action = 0

# Transfer learn by importing Q
    context.Q = defaultdict(lambda: np.zeros(context.num_of_actions))
    context.statecount = defaultdict(int)
    context.actioncount = defaultdict(int)
    context.rewardcount = defaultdict(int)
    context.policy = make_epsilon_greedy_policy(context.Q, context.epsilon, context.num_of_actions)

    context.state = 0#TODO1 Get the initial state
    context.test = 0
    context.previousmoney = context.portfolio.cash
    context.purchasingpower = []
    # Dyna Transition Table
    context.T = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: [0.000000001, 0])))
    context.R = defaultdict(lambda: defaultdict(int))
    
    record(action0 = context.actioncount[0], action1 = context.actioncount[1], action2 = context.actioncount[2], action_now = context.action)
    
    '''
    # Rebalance every day, 1 hour after market open. minutely
    algo.schedule_function(
        rebalance,
        algo.date_rules.every_day(),
        algo.time_rules.market_open(minutes =1, hours=0)
    )
      # Rebalance every day, 1 hour after market open. hourly
    algo.schedule_function(
        rebalance,
        algo.date_rules.every_day(),
        algo.time_rules.market_open(minutes =0, hours=1)
    )
    '''
    '''
     # Rebalance every day, 1 hour after market open. daily
    algo.schedule_function(
        rebalance,
        algo.date_rules.week_start(days_offset=1),
        algo.time_rules.market_open(minutes =30, hours=0)
    )
    '''
    
    # MINUTELY
    for i in range(12):
        for j in range(1, 60):
            algo.schedule_function(rebalance, algo.date_rules.every_day(),  algo.time_rules.market_open(minutes=j, hours=i))

   
    # # HOURLY
    # for i in range(12):
    #     algo.schedule_function(rebalance, date_rules.every_day(), time_rules.market_open(minutes=1, hours=i))
  
    # DAILY
    # algo.schedule_function(rebalance, date_rules.every_day())

    # Record tracking variables at the end of each day.
    algo.schedule_function(
        record_vars,
        algo.date_rules.every_day(),
        algo.time_rules.market_close(),
    )

    # Create our dynamic stock selector.
    algo.attach_pipeline(make_pipeline(), 'pipeline')


def make_pipeline():
    """
    A function to create our dynamic stock selector (pipeline). Documentation
    on pipeline can be found here:
    https://www.quantopian.com/help#pipeline-title
    """

    # Base universe set to the QTradableStocksUS
    base_universe = QTradableStocksUS()

    # Factor of yesterday's close price.
    yesterday_close = USEquityPricing.close.latest

    pipe = Pipeline(
        columns={
            'close': yesterday_close,
        },
        screen=base_universe
    )
    return pipe


def before_trading_start(context, data):
    """
    Called every day before market open.
    """
    context.output = algo.pipeline_output('pipeline')

    # These are the securities that we are interested in trading each day.
    context.security_list = context.output.index


def rebalance(context, data):
    """
    Execute orders according to our schedule_function() timing.
    """
    context.duration = context.duration + 1
    if(data.current(context.my_stock, ["price", "open", "volume", "high","low", "close",]).isnull().values.any()):
        #print("null value")
        #print(data.current(context.my_stock, ["price", "open", "volume", "high","low", "close",]))
        pass
    else:
        #print("notnull value")
        ### Get the Sigmoid to get the number of stocks to buy/sell ####
        numstocks = sigmoid(context.duration) * context.portfolio.cash / data.current(context.my_stock,"price")
        # cur_purchasingpower = context.portfolio.cash / data.current(context.my_stock,"price")
        # context.purchasingpower.append(cur_purchasingpower)
        # if(len(context.purchasingpower) >= 101):
        #     context.purchasingpower.pop(0)
        #############################  Mean Features ########################################
        dp = 10000
        tm = '1m'
        
        context.price_mean = np.mean(data.history(context.my_stock, "price", dp, tm))
        #print("\nMean:", context.price_mean)
        context.openstock_mean = np.mean(data.history(context.my_stock, "open", dp, tm))
        context.volume_mean= np.mean(data.history(context.my_stock,"volume", dp, tm))
        context.high_mean= np.mean(data.history(context.my_stock,"high", dp, tm))
        context.low_mean = np.mean(data.history(context.my_stock,"low", dp, tm))
        context.close_mean = np.mean(data.history(context.my_stock,"close", dp, tm))
    
        #############################  Standard Dev Features ########################################
        context.price_std = np.std(data.history(context.my_stock, "price", dp, tm))
        #log.info(context.price_std)
        context.openstock_std = np.std(data.history(context.my_stock, "open", dp, tm))
        context.volume_std= np.std(data.history(context.my_stock,"volume", dp, tm))
        context.high_std= np.std(data.history(context.my_stock,"high", dp, "1d"))
        context.low_std = np.std(data.history(context.my_stock,"low", dp, "1d"))
        context.close_std = np.std(data.history(context.my_stock,"close", dp, tm))
    
         #####################(######## Discretization Per Feature ########################################
        discretized_price = discretize(context.price_mean, context.price_std, data.current(context.my_stock, "price"))
        discretized_open = discretize(context.openstock_mean, context.openstock_std, data.current(context.my_stock, "open"))
        discretized_volume = discretize(context.volume_mean, context.volume_std, data.current(context.my_stock, "volume"))
        discretized_high = discretize(context.high_mean, context.high_std, data.current(context.my_stock, "high"))
        discretized_low = discretize(context.low_mean, context.low_std, data.current(context.my_stock, "low"))
        discretized_close = discretize(context.close_mean, context.close_std, data.current(context.my_stock, "close"))

        '''
        print("\nSamples:", repr(samples), sep="\n")
        print("\nDiscretized samples:", repr(discretized_samples), sep="\n")
        print("\nState:", discretized_samples[0][1])
        '''
    
        #Starting new tuple
        newtuple = () #Insert Features Here Prices, Volume, High, Low and etc or trading indicators
        newtuple = newtuple + (discretized_price,)
        newtuple = newtuple + (discretized_open,)
        newtuple = newtuple + (discretized_volume,)
        newtuple = newtuple + (discretized_high,)
        newtuple = newtuple + (discretized_low,)
        newtuple = newtuple + (discretized_close,)
        
        # if(len(context.purchasingpower) >= 11):
        #     purchase_mean = np.mean(context.purchasingpower)
        #     purchase_std = np.std(context.purchasingpower)
        #     discretized_purchase = discretize(purchase_mean, purchase_std, cur_purchasingpower)
        #     newtuple = newtuple + (discretized_purchase,)
        # else:
        #     newtuple = newtuple + (0,)
            #print("\nState:", newtuple, sep="\n")
        #order(context.my_stock, numstocks)
    
        if(context.test == 0):
            context.state = newtuple
            context.test = 1
        else:
            action_probs = context.policy(context.state)
            context.action = np.random.choice(np.arange(len(action_probs)), p=action_probs)
#             if this is a random action test with minimum cost
            if action_probs[context.action] < 0.5:
                numstocks = int(numstocks * 0.000000001)
            record(action_now = context.action)
#         action_space = (0, do nothing), (1, buy), (2, sell)
            if context.action == 0:
#             TODO2 do nothing
                pass
            if context.action == 1:
#             TODO3 buy
                order(context.my_stock, numstocks)
            if context.action == 2:
#             TODO4 sell:
                order(context.my_stock, -numstocks)

            # based from the action you did:
            next_state = newtuple#TODO5 get next state/stock data
            #reward = context.portfolio.cash - context.previousmoney #TODO6 compute rewards Profit
            # reward = get_networth(context, data, numstocks)
            reward = get_sharpe_ratio(context, data, numstocks) #rewards Sharpe Rati
            #done = #TODO7 make this true if its on the end of the simulation

            # TD Update
            best_next_action = np.argmax(context.Q[next_state])    
            td_target = reward + context.discount_factor * context.Q[next_state][best_next_action]
            td_delta = td_target - context.Q[context.state][context.action]
            context.Q[context.state][context.action] += context.alpha * td_delta

            # Dyna Transition Table Update
            context.T[context.state][context.action][next_state][0] += 1
            context.R[context.state][context.action] = (1-context.alpha)*context.R[context.state][context.action] + context.alpha*reward
            for s in context.T.keys():
                for a in context.T[s].keys():
                    totalcount = 0
                    for ns in context.T[s][a].keys():
                        totalcount += context.T[s][a][ns][0] 
                    for ns in context.T[s][a].keys():
                        context.T[s][a][ns][1] = context.T[s][a][ns][0]/totalcount
    
        # Dyna Hallucination
            for i in range(context.hallucination):                           
                random_state = random.choice(list(context.T))
                random_action = random.choice(list(context.T[random_state].keys()))
                next_state_list = []
                next_state_transition_probabilties = []
                for ns in context.T[random_state][random_action].keys():
                    next_state_list.append(ns)
                    next_state_transition_probabilties.append(context.T[random_state][random_action][ns][1])
                next_state_index = np.random.choice(range(len(next_state_list)), p=next_state_transition_probabilties) 
                generated_next_state = next_state_list[next_state_index]           
                generated_reward = context.R[random_state][random_action]

                # Dyna Hallucination TD Update
                best_next_action = np.argmax(context.Q[generated_next_state])    
                td_target = generated_reward + context.discount_factor * context.Q[generated_next_state][best_next_action]
                td_delta = td_target - context.Q[random_state][random_action]
                context.Q[random_state][random_action] += context.alpha * td_delta            

            # Tracking
            context.statecount[context.state] += 1
            context.actioncount[context.action] += 1
            context.rewardcount[reward] += 1
            context.previousmoney = context.portfolio.cash

            context.state = next_state
        '''    
        if(context.duration % 100 == 1):
            print("\Action Count", context.actioncount,  sep="\n")
        '''

def record_vars(context, data):
    """
    Plot variables at the end of each day.
    """
    pass


def handle_data(context, data):
    """
    Called every minute.
    """
    '''
    today = get_datetime('US/Eastern')
    if(today.month % 2 == 0 and today.day == 20):
    '''
    '''
    if(context.actioncount[0] + context.actioncount[1] + context.actioncount[2] == 100):
        if(context.temp == 0):
            print("\Action Count", context.actioncount,  sep="\n")
            context.temp = 1
        record(action0 = context.actioncount[0], action1 = context.actioncount[1], action2 = context.actioncount[2])
    elif(context.actioncount[0] + context.actioncount[1] + context.actioncount[2] == 101):
        context.actioncount[0] = 0
        context.actioncount[1] = 0
        context.actioncount[2] = 0
        context.temp = 0
    '''
####### RL Function ######
def make_epsilon_greedy_policy(Q, epsilon, nA):
    def policy_fn(observation):
        A = np.ones(nA, dtype=float) * epsilon / nA 
        best_action = np.argmax(Q[observation])
        if list(Q[observation]).count(Q[observation][0]) == len(Q[observation]):
            best_action = np.random.choice(len(Q[observation]))
        A[best_action] += (1.0 - epsilon)
        return A
    return policy_fn


##### Sigmoid Function #######
def sigmoid(x):
  return 1 / (1 + math.exp(-x))

def get_sharpe_ratio(context, data, numstocks):
    close_data = data.history(context.my_stock,["close","price"], 100, "1d")
    #print( close_data)
    #dividend = close_data[0]
    result = close_data
    #print( result['close'])
    #result = close_data
    result['norm_price'] = result['close'] / result['close'][0]
    result['Allocation'] = result['norm_price'] * 1.000000
    result['Position'] = result['Allocation']* numstocks
    result['Daily Return'] = result['Position'].pct_change(1)
    Sharpe_Ratio = result['Daily Return'].mean() / result['Daily Return'].std()
    #print( Sharpe_Ratio)
    return Sharpe_Ratio

def get_networth(context, data, numstock):
    return context.portfolio.cash + (numstock * data.current(context.my_stock, "close"))

def discretize(mean, std, val):
    disc = 0
    if val <= mean + std*0.5 and val >= mean - std*0.5:
        disc = 0
    elif val > mean + std*0.5 and val <= mean + std*1:
        disc = 1
    elif val > mean + std*1 and val <= mean + std*1.5:
        disc = 2
    elif val > mean + std*1.5 and val <= mean + std*2:
        disc = 3
    elif val > mean + std*2:
        disc = 4
    elif val < mean - std*0.5 and val >= mean - std*1:
        disc = -1
    elif val < mean - std*1 and val >= mean - std*1.5:
        disc = -2
    elif val < mean - std*1.5 and val >= mean - std*2:
        disc = -3
    elif val < mean - std*2:
        disc = -4
    else:
        disc = 0
    return disc